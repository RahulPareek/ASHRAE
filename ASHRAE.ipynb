{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load in \n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# import gc\n",
    "\n",
    "# # Input data files are available in the \"../input/\" directory.\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, PReLU\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "# from tensorflow.keras.layers import Input, Dropout, Dense, Embedding, SpatialDropout1D, concatenate, BatchNormalization, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train = pd.read_csv('../input/ashrae-energy-prediction/train.csv', infer_datetime_format = True)\n",
    "# # weather_train = pd.read_csv('../input/ashrae-energy-prediction/weather_train.csv', infer_datetime_format = True)\n",
    "\n",
    "# building_metadata = pd.read_csv('../input/ashrae-energy-prediction/building_metadata.csv', infer_datetime_format = True)\n",
    "\n",
    "# lgbm_model = '/kaggle/input/lgbm-ashrae/lgb.pkl'\n",
    "# # weights = '/kaggle/input/ashrae6/ashrae.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Memory optimization\n",
    "\n",
    "# # Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n",
    "# # Modified to support timestamp type, categorical type\n",
    "# # Modified to add option to use float16\n",
    "\n",
    "# from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "# from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "# def reduce_mem_usage(df, use_float16=False):\n",
    "#     \"\"\"\n",
    "#     Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
    "#     \"\"\"\n",
    "    \n",
    "#     start_mem = df.memory_usage().sum() / 1024**2\n",
    "#     print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "#     for col in df.columns:\n",
    "#         if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "#             continue\n",
    "#         col_type = df[col].dtype\n",
    "        \n",
    "#         if col_type != object:\n",
    "#             c_min = df[col].min()\n",
    "#             c_max = df[col].max()\n",
    "#             if str(col_type)[:3] == \"int\":\n",
    "#                 if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "#                     df[col] = df[col].astype(np.int8)\n",
    "#                 elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "#                     df[col] = df[col].astype(np.int16)\n",
    "#                 elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "#                     df[col] = df[col].astype(np.int32)\n",
    "#                 elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "#                     df[col] = df[col].astype(np.int64)  \n",
    "#             else:\n",
    "#                 df[col] = df[col].astype(np.float32)\n",
    "#         else:\n",
    "#             df[col] = df[col].astype(\"category\")\n",
    "\n",
    "#     end_mem = df.memory_usage().sum() / 1024**2\n",
    "#     print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "#     print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# def merge_and_preprocess(df, test = False):\n",
    "#     df = pd.merge(df, building_metadata, how = 'left', on = 'building_id')\n",
    "#     if test:\n",
    "#         df = pd.merge(df, weather_test, how = 'left', on = ['site_id', 'timestamp'])\n",
    "#     else:\n",
    "#         df = pd.merge(df, weather_train, how = 'left', on = ['site_id', 'timestamp'])\n",
    "        \n",
    "#     df['square_feet'] = np.log1p(df['square_feet'])\n",
    "    \n",
    "        \n",
    "#     df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "#     df[\"hour\"] = df['timestamp'].dt.hour\n",
    "#     df[\"weekday\"] = df[\"timestamp\"].dt.weekday\n",
    "    \n",
    "#     df = df.drop(columns = ['building_id', 'floor_count', 'year_built', 'precip_depth_1_hr',\n",
    "#                            'timestamp'])\n",
    "    \n",
    "#     return df\n",
    "\n",
    "\n",
    "# #precip_depth_1_hr may not be correct for some site id's\n",
    "# def populate_na(df):\n",
    "# #     df = df.groupby('site_id').\\\n",
    "# #             apply(lambda group: (group.fillna(method='ffill') + group.fillna(method=\"backfill\"))/2)\n",
    "\n",
    "#     cols = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'sea_level_pressure', \n",
    "#         'wind_direction', 'wind_speed']\n",
    "#     for col in cols:\n",
    "#         df[col] = (df[col].fillna(method='ffill') + df[col].fillna(method=\"backfill\"))/2\n",
    "\n",
    "#         df[col] = df[col].fillna(df[col].mean())\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def pre_process(df, test = False):\n",
    "# #     cols_dummies = ['primary_use']\n",
    "# #     df = pd.get_dummies(df, columns = cols_dummies)\n",
    "    \n",
    "#     cols_to_scale = ['meter','square_feet','air_temperature','cloud_coverage','dew_temperature',\\\n",
    "#         'sea_level_pressure','wind_direction','wind_speed']\n",
    "    \n",
    "#     scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "#     for col in cols_to_scale:\n",
    "#         df[col] = scaler.fit_transform(df[col].values.reshape(-1, 1))\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = merge_and_preprocess(train)\n",
    "# train = reduce_mem_usage(train)\n",
    "\n",
    "# del weather_train\n",
    "# gc.collect()\n",
    "\n",
    "# train = populate_na(train)\n",
    "# train['meter_reading'] = np.log1p(train['meter_reading'])\n",
    "# target = train.pop('meter_reading')\n",
    "\n",
    "# train = pre_process(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(train['meter'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(train['floor_count'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_mat = train.corr()\n",
    "# f, ax = plt.subplots(figsize = (12, 9))\n",
    "# sns.heatmap(corr_mat, square = True, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Ratio of available data (not NAN's):\")\n",
    "# data_ratios = train.count()/len(train)\n",
    "# data_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.loc[train['site_id'] == 0,'cloud_coverage'].resample('D').mean().plot(kind='line', figsize = (12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.loc[train['site_id'] == 0,'air_temperature'].resample('D').mean().plot(kind='line', figsize = (12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit_lgbm(seed=None, cat_features=None, num_rounds=1500, lr=0.1, bf=0.1):\n",
    "#     \"\"\"Train Light GBM model\"\"\"\n",
    "#     metric = 'mse'\n",
    "#     params = {'num_leaves': 31,\n",
    "#               'objective': 'regression',\n",
    "# #               'max_depth': -1,\n",
    "#               'learning_rate': lr,\n",
    "#               \"boosting\": \"gbdt\",\n",
    "#               \"bagging_freq\": 5,\n",
    "#               \"bagging_fraction\": bf,\n",
    "#               \"feature_fraction\": 0.9,\n",
    "#               \"metric\": metric,\n",
    "# #               \"verbosity\": -1,\n",
    "# #               'reg_alpha': 0.1,\n",
    "# #               'reg_lambda': 0.3\n",
    "#               }\n",
    "\n",
    "#     params['seed'] = seed\n",
    "\n",
    "#     early_stop = 20\n",
    "#     verbose_eval = 20\n",
    "\n",
    "#     d_train = lgb.Dataset(train, label=target, categorical_feature=cat_features)\n",
    "# #     d_valid = lgb.Dataset(x_val, label=y_val, categorical_feature=cat_features)\n",
    "# #     watchlist = [d_train, d_valid]\n",
    "\n",
    "#     print('training LGB:')\n",
    "#     model = lgb.train(params,\n",
    "#                       train_set=d_train,\n",
    "#                       num_boost_round=num_rounds,\n",
    "#                       verbose_eval=verbose_eval)\n",
    "\n",
    "# #     # predictions\n",
    "# #     y_pred_valid = model.predict(x_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "# #     print('best_score', model.best_score)\n",
    "# #     log = {'train/mae': model.best_score['training']['l2'],\n",
    "# #            'valid/mae': model.best_score['valid_1']['l2']}\n",
    "#     return model\n",
    "\n",
    "# # x_train, x_val , y_train, y_val = train_test_split(train, target, test_size = 0.1)\n",
    "# cat_features = ['primary_use', 'site_id']\n",
    "# model = fit_lgbm(cat_features=cat_features, num_rounds=1000, lr=0.05, bf=0.7)\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.externals import joblib\n",
    "# # save model\n",
    "# # joblib.dump(model, 'lgb.pkl')\n",
    "# # load model\n",
    "# model = joblib.load(lgbm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv('../input/ashrae-energy-prediction/test.csv', infer_datetime_format = True)\n",
    "\n",
    "# weather_test = pd.read_csv('../input/ashrae-energy-prediction/weather_test.csv', infer_datetime_format = True)\n",
    "\n",
    "# row_id = test.pop('row_id')\n",
    "# # sample_submission = pd.read_csv('../input/ashrae-energy-prediction/sample_submission.csv')\n",
    "\n",
    "# test = merge_and_preprocess(test, test = True)\n",
    "# test = reduce_mem_usage(test)\n",
    "\n",
    "# del building_metadata, weather_test\n",
    "\n",
    "# test = populate_na(test)\n",
    "\n",
    "# gc.collect()\n",
    "# test = pre_process(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(istest = False):\n",
    "#     input_size = -1\n",
    "    \n",
    "#     if istest:\n",
    "#         input_size = test.shape[1]\n",
    "#     else:\n",
    "#         input_size = x_train.shape[1]\n",
    "        \n",
    "#     model = Sequential([\n",
    "#         Dense(128, input_shape = (input_size, )),\n",
    "#         PReLU(),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(128),\n",
    "#         PReLU(),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(64),\n",
    "#         PReLU(),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(1)\n",
    "#     ])\n",
    "    \n",
    "#     model.compile(loss = 'mean_squared_error', optimizer = tf.keras.optimizers.Adam(lr = 0.0008),\\\n",
    "#                   metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "    \n",
    "#     return model\n",
    "\n",
    "\n",
    "# def build_complex_model(istest = False):\n",
    "#     input_size = -1\n",
    "    \n",
    "#     if istest:\n",
    "#         input_size = test.shape[1]\n",
    "#     else:\n",
    "#         input_size = x_train.shape[1]\n",
    "        \n",
    "#     model = Sequential([\n",
    "#         Dense(256, input_shape = (input_size, )),\n",
    "#         PReLU(),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(128),\n",
    "#         PReLU(),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(128),\n",
    "#         PReLU(),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(64),\n",
    "#         PReLU(),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(32),\n",
    "#         PReLU(),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(1)\n",
    "#     ])\n",
    "    \n",
    "#     model.compile(loss = 'mean_squared_error', optimizer = tf.keras.optimizers.Adam(lr = 0.1),\\\n",
    "#                   metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_model(False)\n",
    "# model_path = 'ashrae.hdf5'\n",
    "# checkpointer = ModelCheckpoint(filepath = model_path, verbose=1, save_best_only=True)\n",
    "# # # early_stopping = EarlyStopping(monitor = 'val_loss', patience = 7)\n",
    "# # reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)\n",
    "# model.load_weights(weights)\n",
    "# history = model.fit(x_train, y_train, epochs = 12, batch_size = 60, callbacks=[checkpointer],\n",
    "#               validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path_complex = 'ashrae_complex.hdf5' \n",
    "# model = build_complex_model(False)\n",
    "# model_path = 'ashrae.hdf5'\n",
    "# checkpointer = ModelCheckpoint(filepath = model_path_complex, verbose=1, save_best_only=True)\n",
    "# # # early_stopping = EarlyStopping(monitor = 'val_loss', patience = 7)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)\n",
    "# #model.load_weights(model_path_complex)\n",
    "# history = model.fit(x_train, y_train, epochs = 10, batch_size = 60, callbacks=[checkpointer, reduce_lr],\n",
    "#               validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "\n",
    "\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.plot(loss, label='Training Loss')\n",
    "# plt.plot(val_loss, label='Validation Loss')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.ylabel('Cross Entropy')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = pd.concat([row_id, pd.DataFrame(pred.ravel())], axis = 1).\\\n",
    "#                 rename(columns = {0:'meter_reading'})\n",
    "# submission.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 616.95 MB\n",
      "Memory usage after optimization is: 173.90 MB\n",
      "Decreased by 71.8%\n",
      "Memory usage of dataframe is 0.07 MB\n",
      "Memory usage after optimization is: 0.02 MB\n",
      "Decreased by 66.5%\n",
      "Memory usage of dataframe is 9.60 MB\n",
      "Memory usage after optimization is: 4.51 MB\n",
      "Decreased by 53.0%\n",
      "Building model with first half and validating on second half:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:1205: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:762: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 0.926318\tvalid_1's rmse: 1.33896\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-24824db81408>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building model with first half and validating on second half:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m \u001b[0mmodel_half_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_half_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwatchlist_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building model with second half and validating on first half:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1800\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "path_data = \"/kaggle/input/ashrae-energy-prediction/\"\n",
    "path_train = path_data + \"train.csv\"\n",
    "path_test = path_data + \"test.csv\"\n",
    "path_building = path_data + \"building_metadata.csv\"\n",
    "path_weather_train = path_data + \"weather_train.csv\"\n",
    "path_weather_test = path_data + \"weather_test.csv\"\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "myfavouritenumber = 0\n",
    "seed = myfavouritenumber\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "## Memory optimization\n",
    "\n",
    "# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n",
    "# Modified to support timestamp type, categorical type\n",
    "# Modified to add option to use float16\n",
    "\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\"\n",
    "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "        else:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(path_train)\n",
    "\n",
    "building = pd.read_csv(path_building)\n",
    "le = LabelEncoder()\n",
    "building.primary_use = le.fit_transform(building.primary_use)\n",
    "\n",
    "weather_train = pd.read_csv(path_weather_train)\n",
    "\n",
    "\n",
    "df_train = reduce_mem_usage(df_train, use_float16=True)\n",
    "building = reduce_mem_usage(building, use_float16=True)\n",
    "weather_train = reduce_mem_usage(weather_train, use_float16=True)\n",
    "\n",
    "def prepare_data(X, building_data, weather_data, test=False):\n",
    "    \"\"\"\n",
    "    Preparing final dataset with all features.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = X.merge(building_data, on=\"building_id\", how=\"left\")\n",
    "    X = X.merge(weather_data, on=[\"site_id\", \"timestamp\"], how=\"left\")\n",
    "    \n",
    "    X.timestamp = pd.to_datetime(X.timestamp, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    X.square_feet = np.log1p(X.square_feet)\n",
    "    \n",
    "    if not test:\n",
    "        X.sort_values(\"timestamp\", inplace=True)\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    holidays = [\"2016-01-01\", \"2016-01-18\", \"2016-02-15\", \"2016-05-30\", \"2016-07-04\",\n",
    "                \"2016-09-05\", \"2016-10-10\", \"2016-11-11\", \"2016-11-24\", \"2016-12-26\",\n",
    "                \"2017-01-01\", \"2017-01-16\", \"2017-02-20\", \"2017-05-29\", \"2017-07-04\",\n",
    "                \"2017-09-04\", \"2017-10-09\", \"2017-11-10\", \"2017-11-23\", \"2017-12-25\",\n",
    "                \"2018-01-01\", \"2018-01-15\", \"2018-02-19\", \"2018-05-28\", \"2018-07-04\",\n",
    "                \"2018-09-03\", \"2018-10-08\", \"2018-11-12\", \"2018-11-22\", \"2018-12-25\",\n",
    "                \"2019-01-01\"]\n",
    "    \n",
    "    X[\"hour\"] = X.timestamp.dt.hour\n",
    "    X[\"weekday\"] = X.timestamp.dt.weekday\n",
    "    X[\"is_holiday\"] = (X.timestamp.dt.date.astype(\"str\").isin(holidays)).astype(int)\n",
    "    \n",
    "    cols = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'sea_level_pressure', \n",
    "        'wind_direction', 'wind_speed']\n",
    "    \n",
    "    for col in cols:\n",
    "        X[col] = (X[col].fillna(method='ffill') + X[col].fillna(method=\"backfill\"))/2\n",
    "\n",
    "        X[col] = X[col].fillna(X[col].mean())\n",
    "    \n",
    "    drop_features = [\"timestamp\"]\n",
    "\n",
    "    X.drop(drop_features, axis=1, inplace=True)\n",
    "\n",
    "    if test:\n",
    "        row_ids = X.row_id\n",
    "        X.drop(\"row_id\", axis=1, inplace=True)\n",
    "        return X, row_ids\n",
    "    else:\n",
    "        y = np.log1p(X.meter_reading)\n",
    "        X.drop(\"meter_reading\", axis=1, inplace=True)\n",
    "        return X, y\n",
    "    \n",
    "    \n",
    "X_train, y_train = prepare_data(df_train, building, weather_train)\n",
    "\n",
    "del df_train, weather_train\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "X_half_1 = X_train[:int(X_train.shape[0] / 2)]\n",
    "X_half_2 = X_train[int(X_train.shape[0] / 2):]\n",
    "\n",
    "y_half_1 = y_train[:int(X_train.shape[0] / 2)]\n",
    "y_half_2 = y_train[int(X_train.shape[0] / 2):]\n",
    "\n",
    "categorical_features = [\"building_id\", \"site_id\", \"meter\", \"primary_use\", \"hour\", \"weekday\"]\n",
    "\n",
    "d_half_1 = lgb.Dataset(X_half_1, label=y_half_1, categorical_feature=categorical_features, free_raw_data=False)\n",
    "d_half_2 = lgb.Dataset(X_half_2, label=y_half_2, categorical_feature=categorical_features, free_raw_data=False)\n",
    "\n",
    "watchlist_1 = [d_half_1, d_half_2]\n",
    "watchlist_2 = [d_half_2, d_half_1]\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"num_leaves\": 40,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.85,\n",
    "    \"reg_lambda\": 2,\n",
    "    \"metric\": \"rmse\"\n",
    "}\n",
    "\n",
    "print(\"Building model with first half and validating on second half:\")\n",
    "model_half_1 = lgb.train(params, train_set=d_half_1, num_boost_round=1000, valid_sets=watchlist_1, verbose_eval=200, early_stopping_rounds=200)\n",
    "\n",
    "print(\"Building model with second half and validating on first half:\")\n",
    "model_half_2 = lgb.train(params, train_set=d_half_2, num_boost_round=1000, valid_sets=watchlist_2, verbose_eval=200, early_stopping_rounds=200)\n",
    "\n",
    "\n",
    "\n",
    "df_fimp_1 = pd.DataFrame()\n",
    "df_fimp_1[\"feature\"] = X_train.columns.values\n",
    "df_fimp_1[\"importance\"] = model_half_1.feature_importance()\n",
    "df_fimp_1[\"half\"] = 1\n",
    "\n",
    "df_fimp_2 = pd.DataFrame()\n",
    "df_fimp_2[\"feature\"] = X_train.columns.values\n",
    "df_fimp_2[\"importance\"] = model_half_2.feature_importance()\n",
    "df_fimp_2[\"half\"] = 2\n",
    "\n",
    "df_fimp = pd.concat([df_fimp_1, df_fimp_2], axis=0)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=df_fimp.sort_values(by=\"importance\", ascending=False))\n",
    "plt.title(\"LightGBM Feature Importance\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "del X_train, y_train, X_half_1, X_half_2, y_half_1, y_half_2, d_half_1, d_half_2, watchlist_1, watchlist_2, df_fimp_1, df_fimp_2, df_fimp\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(path_test)\n",
    "weather_test = pd.read_csv(path_weather_test)\n",
    "\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "weather_test = reduce_mem_usage(weather_test)\n",
    "\n",
    "X_test, row_ids = prepare_data(df_test, building, weather_test, test=True)\n",
    "\n",
    "del df_test, building, weather_test\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "pred = np.expm1(model_half_1.predict(X_test, num_iteration=model_half_1.best_iteration)) / 2\n",
    "\n",
    "del model_half_1\n",
    "gc.collect()\n",
    "\n",
    "pred += np.expm1(model_half_2.predict(X_test, num_iteration=model_half_2.best_iteration)) / 2\n",
    "    \n",
    "del model_half_2\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "submission = pd.DataFrame({\"row_id\": row_ids, \"meter_reading\": np.clip(pred, 0, a_max=None)})\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
